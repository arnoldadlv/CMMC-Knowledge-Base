# 3.6.3 - Expertise Overlay
## Test the organizational incident response capability

**Source**: Cooey COE Discord - incident-response channel
**Control Status**: 9th most "other than satisfied" requirement per DIBCAC

---

## Sufficient

- **Tabletop exercises (TTX)**: Walk through a realistic scenario discussing how the incident would be handled
- **Checklists and walk-through exercises**: Systematically go through incident response procedures
- **Simulations** (parallel and full interrupt): More advanced testing methods
- **Comprehensive exercises**: Full-scale incident response drills
- **Minimum frequency**: Most assessors expect at least one test per year (no explicit requirement in control text, but assessor expectation)
- **Best practice frequency**: One test per quarter

**Key elements of a sufficient test** (per Amira Armond, C3PAO):
1. Devise a realistic scenario that forces you through all incident plans and procedures
2. Talk through how incident would be handled (discovery, documentation, escalation, containment)
3. Check policies and procedures as they become relevant; try filling out incident response forms
4. Test ability to get help (retained IR firm, cyber insurance, CISO, President, Public Relations)
5. Test ability to contact stakeholders (dibnet submission, customer notification)
6. Test ability to investigate and gather forensics (verify logs would identify malicious activity)
7. Identify lessons learned and start change requests
8. Document everything as evidence

---

## Insufficient

- **Real incidents alone do NOT count as tests** - Explicitly stated: "think that real incidents count as tests - they don't" (Amira Armond)
- **Having policies and plans but not testing them** - DIBCAC specifically identified this as why orgs fail: "orgs were failing because they had policies and plans but didn't test them" (Glenda Snodgrass, Lead CCA)
- **Failing to show improvement from lessons learned** - Listed as a common failure mode

---

## Over-engineering

- **This is NOT technical** - "You do not need special solutions. You do not need whiz-bang incident commandos." (Amira Armond)
- **No special personnel required** - "Your regular system administration team can perform an incident response test. Because guess what - they are the ones that WILL respond to a real incident, at least at the beginning stages."

---

## Edge Cases

### Real Incidents as Testing (Debated)
**Question raised**: Can an organization that follows their plan in a legitimate incident count that as testing?

**Arguments for**: No assessment objective explicitly requires TTX; no AO requires AAR or lessons learned specifically; an actual incident can be an effective test of capability.

**Counter-argument** (Glenda Snodgrass, CCA): The Discussion section lists specific test types (checklists, walk-through, tabletop exercises, simulations). Real incident response can supplement testing but "you still gotta do legit testing."

**Discussion wording noted as ambiguous** by scotty, CCP - could be read two ways regarding whether actual incidents qualify.

**Status**: No definitive resolution in channel; conservative interpretation is to conduct deliberate testing regardless of real incidents.

### Level 1 vs Level 2 Enclave Spills
**Scenario**: Strict Level 2 enclave with Level 1 FCI environment; customer spills CUI via email into Level 1 environment.

**Resolution discussed**:
1. Spills into Level 1 are the fault of the sender - just clean it up
2. Create DLP and other means to identify spills to Level 1 enclave
3. Define IR for Level 1: clean up, notification (if possible), move to Level 2
4. Document in Level 1 policies
5. Make no mention of Level 1 spill handling in Level 2 SSP (out of scope)
6. Still maintain separate IR capability for Level 2 enclave scenarios

---

## Evidence Examples

**Recommended evidence package**:
- Notes from tabletop exercise discussions
- Filled-out incident response forms (tested during exercise)
- Lessons learned documentation
- Change requests initiated from lessons learned
- Modified log settings (if applicable)

**Testing resources mentioned**:
- CISA Tabletop Exercise Packages (CTEPs): https://www.cisa.gov/cisa-tabletop-exercise-packages
- CyberAssist/NDISAC exercises: https://ndisac.org/dibscc/implementation-and-assessment/incident-response-and-management/exercises/
- Washington State Cybersecurity tabletop exercises: https://cybersecurity.wa.gov/tabletop-exercises

**Practical recommendation**: "I recommend having someone on the team who has done a CTT before to help with the process." (Alabaster)

---

## Key Quotes

> "The assessor expectation for 3.6.3 is that the organization has performed a drill or a test of their incident response procedures and capabilities." - Amira Armond, C3PAO

> "It is important to note that there is no actual minimum frequency or schedule in the requirement, however, most assessors will say that the minimum is one test per year. One test per quarter is a best practice for both compliance and security." - Amira Armond, C3PAO

> "This is NOT technical. You do not need special solutions. You do not need whiz-bang incident commandos." - Amira Armond, C3PAO

> "Your regular system administration team can perform an incident response test. Because guess what - they are the ones that WILL respond to a real incident, at least at the beginning stages." - Amira Armond, C3PAO

> "I read that as you can include things you learn in real life response but you still gotta do legit testing." - Glenda Snodgrass, Lead CCA

> "This is one of the Top Ten OTS controls identified by DIBCAC in a slideshow a couple years back and I remember the guy saying specifically that orgs were failing because they had policies and plans but didn't test them." - Glenda Snodgrass, Lead CCA

> "From the Discussion: 'Incident response testing includes the use of checklists, walk-through or tabletop exercises, simulations (both parallel and full interrupt), and comprehensive exercises.'" - Glenda Snodgrass, Lead CCA (quoting NIST discussion)

---

## Common Failure Modes (per Amira Armond)

1. **Thinking real incidents count as tests** - They don't
2. **Getting intimidated by the idea of an incident response test and never doing one**
3. **Failing to show improvement from lessons learned**
